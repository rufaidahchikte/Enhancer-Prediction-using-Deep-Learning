{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a52cc03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lina.utenova/.conda/envs/enhancer-predict/lib/python3.12/inspect.py\n",
      "datasets version: 4.4.1\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "print(inspect.__file__)\n",
    "import datasets\n",
    "print(\"datasets version:\", datasets.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a53d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Downloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Downloading cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Downloading fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl.metadata (112 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Downloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in /home/lina.utenova/.conda/envs/enhancer-predict/lib/python3.12/site-packages (from matplotlib) (2.3.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/lina.utenova/.conda/envs/enhancer-predict/lib/python3.12/site-packages (from matplotlib) (25.0)\n",
      "Collecting pillow>=8 (from matplotlib)\n",
      "  Downloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting pyparsing>=3 (from matplotlib)\n",
      "  Downloading pyparsing-3.2.5-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/lina.utenova/.conda/envs/enhancer-predict/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /home/lina.utenova/.conda/envs/enhancer-predict/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading contourpy-1.3.3-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (362 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Downloading fonttools-4.60.1-cp312-cp312-manylinux1_x86_64.manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_5_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m56.2 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading kiwisolver-1.4.9-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.4 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pillow-12.0.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m99.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.5-py3-none-any.whl (113 kB)\n",
      "Installing collected packages: pyparsing, pillow, kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [matplotlib]7\u001b[0m [matplotlib]\n",
      "\u001b[1A\u001b[2KSuccessfully installed contourpy-1.3.3 cycler-0.12.1 fonttools-4.60.1 kiwisolver-1.4.9 matplotlib-3.10.7 pillow-12.0.0 pyparsing-3.2.5\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Multi-task BERT: Early-stopping, P/R/F1, Saving, and Plots\n",
    "# =========================================\n",
    "import os, json, math, random, re, gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertConfig, BertModel,\n",
    "    TrainingArguments, Trainer, default_data_collator, set_seed,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "\n",
    "!python -m pip install matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9dedfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using LABEL_COL = 'enhancer_label', NUM_TISSUES = 9\n",
      "Train shape: (377626, 706) Val shape: (47203, 706)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b26f8fe2c8542d2b25889e37464e841",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "inferring vocab:   0%|          | 0/377626 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] VOCAB_SIZE = 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/slurm-lina.utenova-98653/ipykernel_475858/3583115195.py:194: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `MultiTaskTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='16236' max='44280' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [16236/44280 40:34 < 1:10:05, 6.67 it/s, Epoch 11/30]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Enh Precision</th>\n",
       "      <th>Enh Recall</th>\n",
       "      <th>Enh F1</th>\n",
       "      <th>Enh Auprc</th>\n",
       "      <th>Enh Auroc</th>\n",
       "      <th>Enh Acc</th>\n",
       "      <th>Tis Precision Micro</th>\n",
       "      <th>Tis Recall Micro</th>\n",
       "      <th>Tis F1 Micro</th>\n",
       "      <th>Tis Precision Macro</th>\n",
       "      <th>Tis Recall Macro</th>\n",
       "      <th>Tis F1 Macro</th>\n",
       "      <th>Tis Auprc Micro</th>\n",
       "      <th>Tis Auprc Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.726900</td>\n",
       "      <td>1.733877</td>\n",
       "      <td>0.816590</td>\n",
       "      <td>0.526530</td>\n",
       "      <td>0.640239</td>\n",
       "      <td>0.822766</td>\n",
       "      <td>0.627159</td>\n",
       "      <td>0.557020</td>\n",
       "      <td>0.214940</td>\n",
       "      <td>0.669977</td>\n",
       "      <td>0.325465</td>\n",
       "      <td>0.207761</td>\n",
       "      <td>0.693320</td>\n",
       "      <td>0.307583</td>\n",
       "      <td>0.301053</td>\n",
       "      <td>0.336324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.703900</td>\n",
       "      <td>1.708794</td>\n",
       "      <td>0.810147</td>\n",
       "      <td>0.565753</td>\n",
       "      <td>0.666245</td>\n",
       "      <td>0.823351</td>\n",
       "      <td>0.625633</td>\n",
       "      <td>0.575663</td>\n",
       "      <td>0.242705</td>\n",
       "      <td>0.599517</td>\n",
       "      <td>0.345529</td>\n",
       "      <td>0.235015</td>\n",
       "      <td>0.630870</td>\n",
       "      <td>0.327662</td>\n",
       "      <td>0.322030</td>\n",
       "      <td>0.349423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.699200</td>\n",
       "      <td>1.696402</td>\n",
       "      <td>0.813148</td>\n",
       "      <td>0.597164</td>\n",
       "      <td>0.688618</td>\n",
       "      <td>0.827086</td>\n",
       "      <td>0.635874</td>\n",
       "      <td>0.595704</td>\n",
       "      <td>0.246907</td>\n",
       "      <td>0.621747</td>\n",
       "      <td>0.353452</td>\n",
       "      <td>0.234237</td>\n",
       "      <td>0.643154</td>\n",
       "      <td>0.332468</td>\n",
       "      <td>0.327351</td>\n",
       "      <td>0.348625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.686200</td>\n",
       "      <td>1.700289</td>\n",
       "      <td>0.810172</td>\n",
       "      <td>0.638311</td>\n",
       "      <td>0.714046</td>\n",
       "      <td>0.826708</td>\n",
       "      <td>0.636773</td>\n",
       "      <td>0.617270</td>\n",
       "      <td>0.239040</td>\n",
       "      <td>0.665292</td>\n",
       "      <td>0.351710</td>\n",
       "      <td>0.226685</td>\n",
       "      <td>0.674457</td>\n",
       "      <td>0.330115</td>\n",
       "      <td>0.328564</td>\n",
       "      <td>0.351873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.679100</td>\n",
       "      <td>1.700356</td>\n",
       "      <td>0.818769</td>\n",
       "      <td>0.580694</td>\n",
       "      <td>0.679481</td>\n",
       "      <td>0.830235</td>\n",
       "      <td>0.641098</td>\n",
       "      <td>0.589878</td>\n",
       "      <td>0.278436</td>\n",
       "      <td>0.549202</td>\n",
       "      <td>0.369528</td>\n",
       "      <td>0.265099</td>\n",
       "      <td>0.569098</td>\n",
       "      <td>0.349264</td>\n",
       "      <td>0.334824</td>\n",
       "      <td>0.356161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.664800</td>\n",
       "      <td>1.694719</td>\n",
       "      <td>0.818068</td>\n",
       "      <td>0.574016</td>\n",
       "      <td>0.674649</td>\n",
       "      <td>0.828737</td>\n",
       "      <td>0.639595</td>\n",
       "      <td>0.585535</td>\n",
       "      <td>0.253212</td>\n",
       "      <td>0.559486</td>\n",
       "      <td>0.348638</td>\n",
       "      <td>0.252487</td>\n",
       "      <td>0.606780</td>\n",
       "      <td>0.335221</td>\n",
       "      <td>0.312281</td>\n",
       "      <td>0.357278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.659700</td>\n",
       "      <td>1.693965</td>\n",
       "      <td>0.800943</td>\n",
       "      <td>0.706879</td>\n",
       "      <td>0.750977</td>\n",
       "      <td>0.828307</td>\n",
       "      <td>0.636668</td>\n",
       "      <td>0.649048</td>\n",
       "      <td>0.253481</td>\n",
       "      <td>0.613712</td>\n",
       "      <td>0.358776</td>\n",
       "      <td>0.243216</td>\n",
       "      <td>0.631119</td>\n",
       "      <td>0.338168</td>\n",
       "      <td>0.332902</td>\n",
       "      <td>0.357515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.667300</td>\n",
       "      <td>1.695645</td>\n",
       "      <td>0.798468</td>\n",
       "      <td>0.743130</td>\n",
       "      <td>0.769806</td>\n",
       "      <td>0.829899</td>\n",
       "      <td>0.639180</td>\n",
       "      <td>0.667288</td>\n",
       "      <td>0.238952</td>\n",
       "      <td>0.664074</td>\n",
       "      <td>0.351445</td>\n",
       "      <td>0.226961</td>\n",
       "      <td>0.682782</td>\n",
       "      <td>0.330325</td>\n",
       "      <td>0.322304</td>\n",
       "      <td>0.352885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.659600</td>\n",
       "      <td>1.690291</td>\n",
       "      <td>0.812088</td>\n",
       "      <td>0.645244</td>\n",
       "      <td>0.719116</td>\n",
       "      <td>0.831135</td>\n",
       "      <td>0.641405</td>\n",
       "      <td>0.622651</td>\n",
       "      <td>0.242364</td>\n",
       "      <td>0.611112</td>\n",
       "      <td>0.347078</td>\n",
       "      <td>0.235815</td>\n",
       "      <td>0.644048</td>\n",
       "      <td>0.329802</td>\n",
       "      <td>0.326050</td>\n",
       "      <td>0.358336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.647000</td>\n",
       "      <td>1.697982</td>\n",
       "      <td>0.806646</td>\n",
       "      <td>0.684863</td>\n",
       "      <td>0.740783</td>\n",
       "      <td>0.831040</td>\n",
       "      <td>0.642300</td>\n",
       "      <td>0.641188</td>\n",
       "      <td>0.222209</td>\n",
       "      <td>0.683282</td>\n",
       "      <td>0.335357</td>\n",
       "      <td>0.214834</td>\n",
       "      <td>0.711095</td>\n",
       "      <td>0.317175</td>\n",
       "      <td>0.319593</td>\n",
       "      <td>0.354825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.644800</td>\n",
       "      <td>1.703817</td>\n",
       "      <td>0.811803</td>\n",
       "      <td>0.648555</td>\n",
       "      <td>0.721055</td>\n",
       "      <td>0.830446</td>\n",
       "      <td>0.641526</td>\n",
       "      <td>0.624346</td>\n",
       "      <td>0.267294</td>\n",
       "      <td>0.551217</td>\n",
       "      <td>0.360012</td>\n",
       "      <td>0.257475</td>\n",
       "      <td>0.581768</td>\n",
       "      <td>0.343065</td>\n",
       "      <td>0.329660</td>\n",
       "      <td>0.353877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved artifacts in: model_out\n",
      " - best_model/ (HF weights + config)\n",
      " - training_meta.json (columns, dims, files)\n",
      " - final_eval_metrics.json\n",
      " - validation_preds_enhancer.csv\n",
      " - curve_*.png (loss, PR/F1, AUPRC, drift²)\n"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# Multi-task BERT (CSV schema with kid_*, mask_*, tissue_*, label)\n",
    "# - Early stopping on F1\n",
    "# - Precision/Recall/F1 metrics\n",
    "# - Save model + results + plots\n",
    "# =========================================\n",
    "import os, json, math, random, re, gc\n",
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, average_precision_score, accuracy_score,\n",
    "    precision_recall_fscore_support\n",
    ")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    BertConfig, BertModel,\n",
    "    TrainingArguments, Trainer, default_data_collator, set_seed,\n",
    "    EarlyStoppingCallback, TrainerCallback, TrainerState, TrainerControl\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Paths & constants\n",
    "# -------------------------\n",
    "TRAIN_CSV = \"train.csv\"\n",
    "VAL_CSV   = \"validation.csv\"\n",
    "OUT_DIR   = \"model_out\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "SEP = \",\"            # CSV delimiter\n",
    "SEED = 42\n",
    "\n",
    "# Sequence layout (no metadata columns)\n",
    "MAX_LEN = 348                       # kid_0..kid_347 and mask_0..mask_347\n",
    "ID_COLS   = [f\"kid_{i}\"  for i in range(MAX_LEN)]\n",
    "MASK_COLS = [f\"mask_{i}\" for i in range(MAX_LEN)]\n",
    "\n",
    "# Model size (compact; adjust as needed)\n",
    "HIDDEN = 256\n",
    "LAYERS = 6\n",
    "HEADS  = 4\n",
    "INTERM = 1024\n",
    "LR = 3e-4\n",
    "EPOCHS_MAX = 30\n",
    "BATCH_TRAIN = 256\n",
    "BATCH_EVAL  = 1024\n",
    "TISSUE_LOSS_WEIGHT = 1.0\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -------------------------\n",
    "# Reproducibility\n",
    "# -------------------------\n",
    "def set_all_seeds(seed=SEED):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "set_all_seeds(SEED)\n",
    "\n",
    "# -------------------------\n",
    "# Column detection (robust to label name)\n",
    "# -------------------------\n",
    "def read_header(path: str) -> List[str]:\n",
    "    return pd.read_csv(path, sep=SEP, nrows=0).columns.tolist()\n",
    "\n",
    "hdr = read_header(TRAIN_CSV)\n",
    "\n",
    "# Ensure kid_* and mask_* exist\n",
    "for cols, name in [(ID_COLS, \"kid_*\"), (MASK_COLS, \"mask_*\")]:\n",
    "    missing = [c for c in cols if c not in hdr]\n",
    "    assert not missing, f\"Missing {name} columns: {missing[:5]}...\"\n",
    "\n",
    "# Detect tissue columns by prefix (tissue_*) or anything named 'tissue...'\n",
    "tissue_cols = [c for c in hdr if c.startswith(\"tissue_\")]\n",
    "if len(tissue_cols) == 0:\n",
    "    tissue_cols = [c for c in hdr if \"tissue\" in c.lower()]\n",
    "assert len(tissue_cols) > 0, \"No tissue columns found (expected tissue_*).\"\n",
    "NUM_TISSUES = len(tissue_cols)\n",
    "\n",
    "# Detect label column robustly:\n",
    "# Try common names; else find a single binary column among remaining that isn't a tissue.\n",
    "LABEL_CANDIDATES = [\"enhancer\", \"enhancer_label\", \"label\", \"target\"]\n",
    "label_col = None\n",
    "for cand in LABEL_CANDIDATES:\n",
    "    if cand in hdr:\n",
    "        label_col = cand\n",
    "        break\n",
    "\n",
    "if label_col is None:\n",
    "    # try infer from sample\n",
    "    candidate_pool = [c for c in hdr if c not in set(ID_COLS) | set(MASK_COLS) | set(tissue_cols)]\n",
    "    sample = pd.read_csv(TRAIN_CSV, sep=SEP, usecols=candidate_pool, nrows=5000, low_memory=False)\n",
    "    def is_binary(s):\n",
    "        v = pd.to_numeric(s, errors=\"coerce\").fillna(0).astype(int)\n",
    "        u = set(v.unique().tolist())\n",
    "        return u.issubset({0,1}) and (0 in u) and (1 in u)\n",
    "    # prefer names containing 'enhanc'\n",
    "    candidates = [c for c in sample.columns if is_binary(sample[c])]\n",
    "    enh_like   = [c for c in candidates if \"enhanc\" in c.lower()]\n",
    "    label_col  = enh_like[0] if len(enh_like)>0 else (candidates[0] if len(candidates)>0 else None)\n",
    "\n",
    "assert label_col is not None, \"Could not find a binary enhancer label column.\"\n",
    "LABEL_COL = label_col\n",
    "print(f\"[INFO] Using LABEL_COL = '{LABEL_COL}', NUM_TISSUES = {NUM_TISSUES}\")\n",
    "\n",
    "# -------------------------\n",
    "# Load CSVs\n",
    "# -------------------------\n",
    "usecols = ID_COLS + MASK_COLS + tissue_cols + [LABEL_COL]\n",
    "dtype_map = {\n",
    "    **{c: \"uint16\" for c in ID_COLS},     # token ids\n",
    "    **{c: \"uint8\"  for c in MASK_COLS},   # attention mask\n",
    "    **{c: \"uint8\"  for c in tissue_cols}, # multi-label tissues\n",
    "    LABEL_COL: \"uint8\",\n",
    "}\n",
    "df_tr = pd.read_csv(TRAIN_CSV, sep=SEP, usecols=usecols, dtype=dtype_map)\n",
    "df_va = pd.read_csv(VAL_CSV,   sep=SEP, usecols=usecols, dtype=dtype_map)\n",
    "print(\"Train shape:\", df_tr.shape, \"Val shape:\", df_va.shape)\n",
    "\n",
    "# -------------------------\n",
    "# Pack to HF Datasets\n",
    "# -------------------------\n",
    "def pack_df(df: pd.DataFrame) -> Dataset:\n",
    "    arr_ids  = df[ID_COLS].to_numpy(dtype=np.int64)\n",
    "    arr_mask = df[MASK_COLS].to_numpy(dtype=np.int64)\n",
    "    arr_lab  = df[LABEL_COL].to_numpy(dtype=np.int64)\n",
    "    arr_tis  = df[tissue_cols].to_numpy(dtype=np.int64)\n",
    "    recs = [{\n",
    "        \"input_ids\":      arr_ids[i].tolist(),\n",
    "        \"attention_mask\": arr_mask[i].tolist(),\n",
    "        \"labels\":         int(arr_lab[i]),\n",
    "        \"tissues\":        arr_tis[i].tolist(),\n",
    "    } for i in range(len(df))]\n",
    "    return Dataset.from_list(recs)\n",
    "\n",
    "ds_train = pack_df(df_tr)\n",
    "ds_val   = pack_df(df_va)\n",
    "\n",
    "# Infer vocab size from kid_* values\n",
    "def _per_max(batch):\n",
    "    return {\"max_id\": [int(np.max(ids)) for ids in batch[\"input_ids\"]]}\n",
    "tmp = ds_train.map(_per_max, batched=True, batch_size=10_000, desc=\"inferring vocab\")\n",
    "VOCAB_SIZE = int(max(tmp[\"max_id\"])) + 1\n",
    "print(f\"[INFO] VOCAB_SIZE = {VOCAB_SIZE}\")\n",
    "\n",
    "# -------------------------\n",
    "# Model\n",
    "# -------------------------\n",
    "class MultiTaskBERT(nn.Module):\n",
    "    def __init__(self, vocab_size, num_tissues):\n",
    "        super().__init__()\n",
    "        config = BertConfig(\n",
    "            vocab_size=vocab_size,\n",
    "            hidden_size=HIDDEN,\n",
    "            num_hidden_layers=LAYERS,\n",
    "            num_attention_heads=HEADS,\n",
    "            intermediate_size=INTERM,\n",
    "            max_position_embeddings=MAX_LEN,\n",
    "            type_vocab_size=1,\n",
    "            hidden_dropout_prob=0.1,\n",
    "            attention_probs_dropout_prob=0.1\n",
    "        )\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.enhancer_head = nn.Linear(HIDDEN, 2)\n",
    "        self.tissue_head   = nn.Linear(HIDDEN, num_tissues)\n",
    "\n",
    "    def masked_mean_pool(self, hidden_states, attention_mask):\n",
    "        mask = attention_mask.unsqueeze(-1).float()\n",
    "        summed = (hidden_states * mask).sum(dim=1)\n",
    "        denom = mask.sum(dim=1).clamp(min=1e-6)\n",
    "        return summed / denom\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None, tissues=None):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.masked_mean_pool(out.last_hidden_state, attention_mask)\n",
    "        pooled = self.dropout(pooled)\n",
    "        logits_enh = self.enhancer_head(pooled)\n",
    "        logits_tis = self.tissue_head(pooled)\n",
    "        return {\"logits_enh\": logits_enh, \"logits_tis\": logits_tis}\n",
    "\n",
    "# -------------------------\n",
    "# Custom Trainer (weighted multitask loss)\n",
    "# -------------------------\n",
    "class MultiTaskTrainer(Trainer):\n",
    "    def __init__(self, class_weights_ce=None, pos_weight_bce=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.class_weights_ce = class_weights_ce\n",
    "        self.pos_weight_bce   = pos_weight_bce\n",
    "\n",
    "    # NOTE: accept **kwargs to swallow num_items_in_batch and any future args\n",
    "    def compute_loss(self, model, inputs, return_outputs: bool = False, **kwargs):\n",
    "        labels  = inputs.pop(\"labels\")\n",
    "        tissues = inputs.pop(\"tissues\")\n",
    "        outputs = model(**inputs)\n",
    "        logits_enh = outputs[\"logits_enh\"]\n",
    "        logits_tis = outputs[\"logits_tis\"]\n",
    "\n",
    "        ce = nn.CrossEntropyLoss(weight=self.class_weights_ce)\n",
    "        loss_enh = ce(logits_enh, labels)\n",
    "\n",
    "        enh_mask = (labels == 1)\n",
    "        if enh_mask.any():\n",
    "            logits_pos  = logits_tis[enh_mask]\n",
    "            tissues_pos = tissues[enh_mask].float()\n",
    "            bce = nn.BCEWithLogitsLoss(pos_weight=self.pos_weight_bce)\n",
    "            loss_tis = bce(logits_pos, tissues_pos)\n",
    "        else:\n",
    "            loss_tis = torch.tensor(0.0, device=logits_tis.device)\n",
    "\n",
    "        loss = loss_enh + TISSUE_LOSS_WEIGHT * loss_tis\n",
    "        return (loss, {\"logits_enh\": logits_enh, \"logits_tis\": logits_tis}) if return_outputs else loss\n",
    "\n",
    "# -------------------------\n",
    "# Metrics (adds Precision/Recall/F1)\n",
    "# -------------------------\n",
    "def compute_metrics_fn(eval_pred):\n",
    "    preds, labels = eval_pred\n",
    "    if isinstance(preds, dict):\n",
    "        logits_enh = preds[\"logits_enh\"]\n",
    "        logits_tis = preds[\"logits_tis\"]\n",
    "    else:\n",
    "        logits_enh, logits_tis = preds\n",
    "\n",
    "    y_enh, y_tis = labels  # due to label_names\n",
    "\n",
    "    logits_enh = np.asarray(logits_enh)\n",
    "    logits_tis = np.asarray(logits_tis)\n",
    "    y_enh      = np.asarray(y_enh)\n",
    "    y_tis      = np.asarray(y_tis)\n",
    "\n",
    "    p_enh = (torch.softmax(torch.tensor(logits_enh), dim=-1).numpy()[:, 1])\n",
    "    y_pred_enh = (p_enh >= 0.5).astype(int)\n",
    "\n",
    "    enh_prec, enh_rec, enh_f1, _ = precision_recall_fscore_support(\n",
    "        y_enh, y_pred_enh, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    try:\n",
    "        enh_auprc = average_precision_score(y_enh, p_enh)\n",
    "        enh_auroc = roc_auc_score(y_enh, p_enh)\n",
    "    except Exception:\n",
    "        enh_auprc, enh_auroc = float(\"nan\"), float(\"nan\")\n",
    "    enh_acc = accuracy_score(y_enh, y_pred_enh)\n",
    "\n",
    "    mask = (y_enh == 1)\n",
    "    if mask.sum() > 0:\n",
    "        y_true_t = y_tis[mask].astype(int)\n",
    "        y_prob_t = torch.sigmoid(torch.tensor(logits_tis)).numpy()[mask]\n",
    "        y_pred_t = (y_prob_t >= 0.5).astype(int)\n",
    "\n",
    "        tis_prec_micro, tis_rec_micro, tis_f1_micro, _ = precision_recall_fscore_support(\n",
    "            y_true_t, y_pred_t, average=\"micro\", zero_division=0\n",
    "        )\n",
    "        tis_prec_macro, tis_rec_macro, tis_f1_macro, _ = precision_recall_fscore_support(\n",
    "            y_true_t, y_pred_t, average=\"macro\", zero_division=0\n",
    "        )\n",
    "        try:\n",
    "            tis_auprc_micro = average_precision_score(y_true_t.reshape(-1), y_prob_t.reshape(-1))\n",
    "        except Exception:\n",
    "            tis_auprc_micro = float(\"nan\")\n",
    "        per_t = []\n",
    "        for j in range(y_true_t.shape[1]):\n",
    "            yj = y_true_t[:, j]; pj = y_prob_t[:, j]\n",
    "            if (yj.sum() > 0) and (yj.sum() < len(yj)):\n",
    "                per_t.append(average_precision_score(yj, pj))\n",
    "        tis_auprc_macro = float(np.mean(per_t)) if per_t else float(\"nan\")\n",
    "    else:\n",
    "        tis_prec_micro = tis_rec_micro = tis_f1_micro = 0.0\n",
    "        tis_prec_macro = tis_rec_macro = tis_f1_macro = 0.0\n",
    "        tis_auprc_micro = tis_auprc_macro = 0.0\n",
    "\n",
    "    return {\n",
    "        \"enh_precision\": float(enh_prec),\n",
    "        \"enh_recall\":    float(enh_rec),\n",
    "        \"enh_f1\":        float(enh_f1),\n",
    "        \"enh_auprc\":     float(enh_auprc),\n",
    "        \"enh_auroc\":     float(enh_auroc),\n",
    "        \"enh_acc\":       float(enh_acc),\n",
    "\n",
    "        \"tis_precision_micro\": float(tis_prec_micro),\n",
    "        \"tis_recall_micro\":    float(tis_rec_micro),\n",
    "        \"tis_f1_micro\":        float(tis_f1_micro),\n",
    "        \"tis_precision_macro\": float(tis_prec_macro),\n",
    "        \"tis_recall_macro\":    float(tis_rec_macro),\n",
    "        \"tis_f1_macro\":        float(tis_f1_macro),\n",
    "        \"tis_auprc_micro\":     float(tis_auprc_micro),\n",
    "        \"tis_auprc_macro\":     float(tis_auprc_macro),\n",
    "    }\n",
    "\n",
    "# -------------------------\n",
    "# Param Drift² callback\n",
    "# -------------------------\n",
    "class ParamDriftCallback(TrainerCallback):\n",
    "    def __init__(self):\n",
    "        self.prev = None\n",
    "    def on_epoch_end(self, args, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        model = kwargs[\"model\"]\n",
    "        vec = torch.cat([p.detach().float().flatten().cpu()\n",
    "                         for p in model.parameters() if p.requires_grad])\n",
    "        if self.prev is None:\n",
    "            drift2 = 0.0\n",
    "        else:\n",
    "            diff = vec - self.prev\n",
    "            drift2 = float((diff * diff).sum().item())\n",
    "        self.prev = vec\n",
    "        state.log_history.append({\"epoch\": state.epoch, \"param_drift2\": drift2})\n",
    "\n",
    "param_drift_cb = ParamDriftCallback()\n",
    "\n",
    "# -------------------------\n",
    "# Class weights\n",
    "# -------------------------\n",
    "y_tr = df_tr[LABEL_COL].to_numpy()\n",
    "n_pos = int((y_tr == 1).sum()); n_neg = int((y_tr == 0).sum())\n",
    "w_neg = 0.5 * (n_pos + n_neg) / max(n_neg, 1)\n",
    "w_pos = 0.5 * (n_pos + n_neg) / max(n_pos, 1)\n",
    "class_weights = torch.tensor([w_neg, w_pos], dtype=torch.float, device=DEVICE)\n",
    "\n",
    "tis_tr = df_tr[tissue_cols].to_numpy()\n",
    "enh_mask_tr = (y_tr == 1)\n",
    "if enh_mask_tr.any():\n",
    "    pos_counts = tis_tr[enh_mask_tr].sum(axis=0) + 1e-6\n",
    "    neg_counts = enh_mask_tr.sum() - pos_counts + 1e-6\n",
    "    pos_weight = torch.tensor(neg_counts / pos_counts, dtype=torch.float, device=DEVICE)\n",
    "else:\n",
    "    pos_weight = torch.ones(NUM_TISSUES, dtype=torch.float, device=DEVICE)\n",
    "\n",
    "# -------------------------\n",
    "# Trainer setup (early stopping on F1)\n",
    "# -------------------------\n",
    "model = MultiTaskBERT(vocab_size=VOCAB_SIZE, num_tissues=NUM_TISSUES).to(DEVICE)\n",
    "fp16_flag = torch.cuda.is_available()\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=os.path.join(OUT_DIR, \"checkpoints\"),\n",
    "    per_device_train_batch_size=BATCH_TRAIN,\n",
    "    per_device_eval_batch_size=BATCH_EVAL,\n",
    "    learning_rate=LR,\n",
    "    num_train_epochs=EPOCHS_MAX,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    label_names=[\"labels\",\"tissues\"],\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_enh_f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=fp16_flag,\n",
    "    dataloader_num_workers=4,\n",
    "    dataloader_pin_memory=True,\n",
    "    dataloader_persistent_workers=False,\n",
    "    logging_steps=100,\n",
    "    seed=SEED,\n",
    "    report_to=[],  # add \"tensorboard\" to log to TB\n",
    ")\n",
    "\n",
    "early_stop_cb = EarlyStoppingCallback(\n",
    "    early_stopping_patience=3, early_stopping_threshold=0.0\n",
    ")\n",
    "\n",
    "trainer = MultiTaskTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=ds_train,\n",
    "    eval_dataset=ds_val,\n",
    "    data_collator=default_data_collator,\n",
    "    class_weights_ce=class_weights,\n",
    "    pos_weight_bce=pos_weight,\n",
    "    tokenizer=None,\n",
    "    compute_metrics=compute_metrics_fn,\n",
    "    callbacks=[early_stop_cb, param_drift_cb],\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "final_eval = trainer.evaluate()\n",
    "\n",
    "# -------------------------\n",
    "# Save artifacts (shareable)\n",
    "# -------------------------\n",
    "trainer.save_model(os.path.join(OUT_DIR, \"best_model\"))  # weights + config.json\n",
    "\n",
    "meta = {\n",
    "    \"VOCAB_SIZE\": VOCAB_SIZE,\n",
    "    \"MAX_LEN\": MAX_LEN,\n",
    "    \"MODEL_DIMS\": {\"HIDDEN\": HIDDEN, \"LAYERS\": LAYERS, \"HEADS\": HEADS, \"INTERM\": INTERM},\n",
    "    \"TRAIN_ARGS\": {\n",
    "        \"LR\": LR, \"BATCH_TRAIN\": BATCH_TRAIN, \"BATCH_EVAL\": BATCH_EVAL,\n",
    "        \"TISSUE_LOSS_WEIGHT\": TISSUE_LOSS_WEIGHT, \"SEED\": SEED\n",
    "    },\n",
    "    \"COLUMNS\": {\n",
    "        \"LABEL_COL\": LABEL_COL,\n",
    "        \"ID_COLS\": ID_COLS,\n",
    "        \"MASK_COLS\": MASK_COLS,\n",
    "        \"TISSUE_COLS\": tissue_cols\n",
    "    },\n",
    "    \"FILES\": {\"train\": TRAIN_CSV, \"validation\": VAL_CSV}\n",
    "}\n",
    "with open(os.path.join(OUT_DIR, \"training_meta.json\"), \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "with open(os.path.join(OUT_DIR, \"final_eval_metrics.json\"), \"w\") as f:\n",
    "    json.dump(final_eval, f, indent=2)\n",
    "\n",
    "# Validation predictions snapshot\n",
    "pred_out = trainer.predict(ds_val)\n",
    "preds, labels = pred_out.predictions, pred_out.label_ids\n",
    "if isinstance(preds, dict):\n",
    "    logits_enh, logits_tis = preds[\"logits_enh\"], preds[\"logits_tis\"]\n",
    "else:\n",
    "    logits_enh, logits_tis = preds\n",
    "y_enh, y_tis = labels\n",
    "\n",
    "p_enh = torch.softmax(torch.tensor(logits_enh), dim=-1).numpy()[:,1]\n",
    "y_pred_enh = (p_enh >= 0.5).astype(int)\n",
    "pred_df = pd.DataFrame({\n",
    "    \"y_true_enh\": y_enh.astype(int),\n",
    "    \"p_enh\": p_enh,\n",
    "    \"y_pred_enh\": y_pred_enh\n",
    "})\n",
    "pred_df.to_csv(os.path.join(OUT_DIR, \"validation_preds_enhancer.csv\"), index=False)\n",
    "\n",
    "# -------------------------\n",
    "# Plots from log history\n",
    "# -------------------------\n",
    "hist = trainer.state.log_history\n",
    "\n",
    "# Loss curves\n",
    "epochs = []; train_loss = []; eval_loss = []\n",
    "for h in hist:\n",
    "    if \"loss\" in h and \"epoch\" in h:\n",
    "        epochs.append(h[\"epoch\"]); train_loss.append(h[\"loss\"])\n",
    "    if \"eval_loss\" in h and \"epoch\" in h:\n",
    "        eval_loss.append((h[\"epoch\"], h[\"eval_loss\"]))\n",
    "\n",
    "plt.figure()\n",
    "if epochs:\n",
    "    plt.plot(epochs, train_loss, marker=\"o\", label=\"train_loss\")\n",
    "if eval_loss:\n",
    "    evx = [e for e,_ in eval_loss]\n",
    "    evy = [l for _,l in eval_loss]\n",
    "    plt.plot(evx, evy, marker=\"o\", label=\"eval_loss\")\n",
    "plt.xlabel(\"Epoch\"); plt.ylabel(\"Loss\"); plt.title(\"Loss per epoch\")\n",
    "plt.legend(); plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUT_DIR, \"curve_loss.png\")); plt.close()\n",
    "\n",
    "# Metric curves\n",
    "def plot_metric(metric_key, title, fname):\n",
    "    ex = []; ey = []\n",
    "    for h in hist:\n",
    "        if metric_key in h and \"epoch\" in h:\n",
    "            ex.append(h[\"epoch\"]); ey.append(h[metric_key])\n",
    "    if ex:\n",
    "        plt.figure()\n",
    "        plt.plot(ex, ey, marker=\"o\")\n",
    "        plt.xlabel(\"Epoch\"); plt.ylabel(metric_key)\n",
    "        plt.title(title)\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(os.path.join(OUT_DIR, fname)); plt.close()\n",
    "\n",
    "plot_metric(\"eval_enh_precision\", \"Enhancer Precision (val)\", \"curve_enh_precision.png\")\n",
    "plot_metric(\"eval_enh_recall\",    \"Enhancer Recall (val)\",    \"curve_enh_recall.png\")\n",
    "plot_metric(\"eval_enh_f1\",        \"Enhancer F1 (val)\",        \"curve_enh_f1.png\")\n",
    "plot_metric(\"eval_enh_auprc\",     \"Enhancer AUPRC (val)\",     \"curve_enh_auprc.png\")\n",
    "plot_metric(\"eval_tis_f1_micro\",  \"Tissues F1 micro (val)\",   \"curve_tis_f1_micro.png\")\n",
    "plot_metric(\"eval_tis_f1_macro\",  \"Tissues F1 macro (val)\",   \"curve_tis_f1_macro.png\")\n",
    "\n",
    "# Param drift² per epoch\n",
    "ex = []; ey = []\n",
    "for h in hist:\n",
    "    if \"param_drift2\" in h and \"epoch\" in h:\n",
    "        ex.append(h[\"epoch\"]); ey.append(h[\"param_drift2\"])\n",
    "if ex:\n",
    "    plt.figure()\n",
    "    plt.plot(ex, ey, marker=\"o\")\n",
    "    plt.xlabel(\"Epoch\"); plt.ylabel(\"Param drift²\")\n",
    "    plt.title(\"Sum of squared parameter change per epoch\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(OUT_DIR, \"curve_param_drift2.png\")); plt.close()\n",
    "\n",
    "print(\"\\nSaved artifacts in:\", OUT_DIR)\n",
    "print(\" - best_model/ (HF weights + config)\")\n",
    "print(\" - training_meta.json (columns, dims, files)\")\n",
    "print(\" - final_eval_metrics.json\")\n",
    "print(\" - validation_preds_enhancer.csv\")\n",
    "print(\" - curve_*.png (loss, PR/F1, AUPRC, drift²)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f4d5ecf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After training (or reload model from the Trainer)\n",
    "cfg = model.bert.config\n",
    "cfg.save_pretrained(\"model_out/best_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0890ed55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertConfig\n",
    "import torch, json, pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "OUT_DIR = \"model_out\"\n",
    "with open(f\"{OUT_DIR}/training_meta.json\") as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "ID_COLS    = meta[\"COLUMNS\"][\"ID_COLS\"]\n",
    "MASK_COLS  = meta[\"COLUMNS\"][\"MASK_COLS\"]\n",
    "LABEL_COL  = meta[\"COLUMNS\"][\"LABEL_COL\"]\n",
    "tissue_cols= meta[\"COLUMNS\"][\"TISSUE_COLS\"]\n",
    "MAX_LEN    = meta[\"MAX_LEN\"]\n",
    "\n",
    "# Rebuild model and load weights\n",
    "from transformers import AutoConfig\n",
    "cfg = BertConfig.from_pretrained(f\"{OUT_DIR}/best_model\")\n",
    "class MultiTaskBERT(nn.Module):\n",
    "    def __init__(self, config, num_tissues):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel(config)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.enhancer_head = nn.Linear(config.hidden_size, 2)\n",
    "        self.tissue_head   = nn.Linear(config.hidden_size, len(tissue_cols))\n",
    "    def masked_mean_pool(self, hs, am):\n",
    "        mask = am.unsqueeze(-1).float()\n",
    "        return (hs*mask).sum(1) / mask.sum(1).clamp(min=1e-6)\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = self.dropout(self.masked_mean_pool(out.last_hidden_state, attention_mask))\n",
    "        return {\n",
    "            \"logits_enh\": self.enhancer_head(pooled),\n",
    "            \"logits_tis\": self.tissue_head(pooled)\n",
    "        }\n",
    "\n",
    "model = MultiTaskBERT(cfg, num_tissues=len(tissue_cols))\n",
    "state = torch.load(f\"{OUT_DIR}/best_model/pytorch_model.bin\", map_location=\"cpu\")\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# Prepare test the same way:\n",
    "def pack_df(df):\n",
    "    return Dataset.from_list([{\n",
    "        \"input_ids\":[int(df.loc[i,c]) for c in ID_COLS],\n",
    "        \"attention_mask\":[int(df.loc[i,c]) for c in MASK_COLS],\n",
    "        # labels/tissues optional for test\n",
    "    } for i in range(len(df))])\n",
    "\n",
    "test = pd.read_csv(\"test.csv\", usecols=ID_COLS+MASK_COLS, dtype={**{c:\"uint16\" for c in ID_COLS}, **{c:\"uint8\" for c in MASK_COLS}})\n",
    "ds_test = pack_df(test)\n",
    "# then build a DataLoader and run forward passes to get logits -> probs\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enhancer-predict",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
